{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b377c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kitchen_generation.py\n",
    "\n",
    "import os, cv2, numpy as np, gc, torch\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from diffusers import (\n",
    "    ControlNetModel, AutoencoderKL, StableDiffusionXLControlNetPipeline,\n",
    "    StableDiffusionXLImg2ImgPipeline, UNet2DConditionModel, EulerDiscreteScheduler\n",
    ")\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPTextModelWithProjection\n",
    "from diffusers.utils import load_image\n",
    "import cohere\n",
    "\n",
    "# === Initial setup ===\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "os.environ[\"TMPDIR\"] = \"/workspace/tmp\"\n",
    "os.environ[\"XFORMERS_DISABLED\"] = \"1\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "\n",
    "# === Paths ===\n",
    "CONTROLNET_CANNY_DIR = \"/workspace/stable-diffusion-webui/models/ControlNet/controlnet-canny-sdxl\"\n",
    "CONTROLNET_DEPTH_DIR = \"/workspace/stable-diffusion-webui/models/ControlNet/controlnet-depth-sdxl\"\n",
    "MODEL_BASE = \"/workspace/stable-diffusion-webui/models/sd_xl_base_1.0\"\n",
    "REFINER_DIR = \"/workspace/stable-diffusion-webui/models/sd_xl_refiner_1.0\"\n",
    "VAE_PATH = f\"{MODEL_BASE}/vae\"\n",
    "OUTPUT_DIR = \"/workspace/stable-diffusion-webui/outputs\"\n",
    "\n",
    "# === Load once ===\n",
    "print(\"ðŸ”¹ Loading models...\")\n",
    "\n",
    "co = cohere.Client(\"V2CGAFrhoVGvMmqOwhxrH7uimhIgR2S8787ODJD4\")\n",
    "controlnet_canny = ControlNetModel.from_pretrained(CONTROLNET_CANNY_DIR, torch_dtype=torch.float16)\n",
    "controlnet_depth = ControlNetModel.from_pretrained(CONTROLNET_DEPTH_DIR, torch_dtype=torch.float16)\n",
    "vae = AutoencoderKL.from_pretrained(VAE_PATH, torch_dtype=torch.float16, variant=\"fp16\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(f\"{MODEL_BASE}/tokenizer\")\n",
    "tokenizer_2 = CLIPTokenizer.from_pretrained(f\"{MODEL_BASE}/tokenizer_2\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(f\"{MODEL_BASE}/text_encoder\", torch_dtype=torch.float16)\n",
    "text_encoder_2 = CLIPTextModelWithProjection.from_pretrained(f\"{MODEL_BASE}/text_encoder_2\", torch_dtype=torch.float16)\n",
    "unet = UNet2DConditionModel.from_pretrained(f\"{MODEL_BASE}/unet\", torch_dtype=torch.float16, variant=\"fp16\")\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(f\"{MODEL_BASE}/scheduler\")\n",
    "\n",
    "pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    pretrained_model_name_or_path=MODEL_BASE,\n",
    "    controlnet=[controlnet_canny, controlnet_depth],\n",
    "    unet=unet, vae=vae,\n",
    "    text_encoder=text_encoder, text_encoder_2=text_encoder_2,\n",
    "    tokenizer=tokenizer, tokenizer_2=tokenizer_2,\n",
    "    scheduler=scheduler,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "pipe.enable_attention_slicing()\n",
    "pipe.enable_vae_tiling()\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "    REFINER_DIR,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\"\n",
    ")\n",
    "refiner.enable_model_cpu_offload()\n",
    "refiner.enable_attention_slicing()\n",
    "refiner.enable_vae_tiling()\n",
    "\n",
    "# === Function ===\n",
    "def run_kitchen_generation(transcript, input_image_path):\n",
    "    # ðŸ§  Prompt Engineering\n",
    "    print(\"ðŸŽ¯ Extracting SDXL Prompt from transcript...\")\n",
    "    system_prompt = \"\"\"\n",
    "You are a kitchen prompt expert. Your job is to read a conversation transcript and generate a short, visually rich prompt that can be used with Stable Diffusion XL (SDXL).\n",
    "\n",
    "Rules:\n",
    "- Format: \"[Style] kitchen, [materials], [layout/furniture], [lighting], photorealistic\"\n",
    "- Do NOT use vague words like \"beautiful\" or \"cozy\"\n",
    "- Focus on visual elements only (e.g. colors, materials, objects, layout)\n",
    "- Max 77 tokens\n",
    "- No user names, no quotes, no assistant text\n",
    "\"\"\"\n",
    "    full_prompt = system_prompt + \"\\n\\nTranscript:\\n\" + transcript + \"\\n\\nOutput:\"\n",
    "    response = co.generate(model=\"command-r-plus\", prompt=full_prompt, max_tokens=100, temperature=0.6)\n",
    "    PROMPT = response.generations[0].text.strip()\n",
    "    print(f\"âœ… Prompt: {PROMPT}\")\n",
    "    NEG_PROMPT = \"blurry, low quality, distorted, granite countertops, generic kitchen, plain design\"\n",
    "\n",
    "    # ðŸ–¼ Image Preprocessing\n",
    "    init_image = load_image(input_image_path).convert(\"RGB\")\n",
    "    width, height = init_image.size\n",
    "    gray = cv2.cvtColor(np.array(init_image), cv2.COLOR_RGB2GRAY)\n",
    "    canny = cv2.Canny(gray, 100, 200)\n",
    "    canny_image = Image.fromarray(cv2.cvtColor(canny, cv2.COLOR_GRAY2RGB))\n",
    "    depth = init_image.convert(\"L\")\n",
    "    depth_array = cv2.equalizeHist(np.array(depth))\n",
    "    depth_image = Image.fromarray(np.stack([depth_array]*3, axis=-1))\n",
    "\n",
    "    # ðŸ—‚ Save debug images\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_run_dir = os.path.join(OUTPUT_DIR, f\"run_{timestamp}\")\n",
    "    os.makedirs(output_run_dir, exist_ok=True)\n",
    "    init_image.save(os.path.join(output_run_dir, \"01_original.png\"))\n",
    "    canny_image.save(os.path.join(output_run_dir, \"02_canny.png\"))\n",
    "    depth_image.save(os.path.join(output_run_dir, \"03_depth.png\"))\n",
    "\n",
    "    # ðŸŽ¨ Generation\n",
    "    print(\"ðŸŽ¨ Generating with ControlNet...\")\n",
    "    result = pipe(\n",
    "        prompt=PROMPT,\n",
    "        negative_prompt=NEG_PROMPT,\n",
    "        image=[canny_image, depth_image],\n",
    "        controlnet_conditioning_scale=[0.5, 0.2],\n",
    "        num_inference_steps=40,\n",
    "        guidance_scale=9.0,\n",
    "        width=width,\n",
    "        height=height\n",
    "    ).images[0]\n",
    "    result_path = os.path.join(output_run_dir, \"04_result.png\")\n",
    "    result.save(result_path)\n",
    "\n",
    "    # âœ¨ Refinement\n",
    "    print(\"âœ¨ Refining with SDXL Refiner...\")\n",
    "    refined = refiner(\n",
    "        prompt=PROMPT,\n",
    "        negative_prompt=NEG_PROMPT,\n",
    "        image=result,\n",
    "        strength=0.3,\n",
    "        guidance_scale=7.5,\n",
    "        num_inference_steps=25\n",
    "    ).images[0]\n",
    "    refined_path = os.path.join(output_run_dir, \"05_refined.png\")\n",
    "    refined.save(refined_path)\n",
    "\n",
    "    print(f\"âœ… Refined image saved to: {refined_path}\")\n",
    "    return refined_path"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
