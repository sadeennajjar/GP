{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e626e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from diffusers import (\n",
    "    StableDiffusionXLControlNetPipeline,\n",
    "    ControlNetModel,\n",
    "    AutoencoderKL,\n",
    "    EulerDiscreteScheduler\n",
    ")\n",
    "from diffusers.utils import load_image\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPConfig\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "from transformers import CLIPTextModelWithProjection\n",
    "import os, gc, torch\n",
    "import cohere\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"TMPDIR\"] = \"/workspace/tmp\"\n",
    "os.environ[\"XFORMERS_DISABLED\"] = \"1\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "\n",
    "# === Paths ===\n",
    "CONTROLNET_CANNY_DIR = \"/workspace/stable-diffusion-webui/models/ControlNet/controlnet-canny-sdxl\"\n",
    "CONTROLNET_DEPTH_DIR = \"/workspace/stable-diffusion-webui/models/ControlNet/controlnet-depth-sdxl\"\n",
    "MODEL_BASE = \"/workspace/stable-diffusion-webui/models/sd_xl_base_1.0\"\n",
    "VAE_PATH = f\"{MODEL_BASE}/vae\"\n",
    "\n",
    "INPUT_IMAGE = \"/workspace/stable-diffusion-webui/images/depositphotos_5894774-stock-photo-empty-kitchen.jpg\"\n",
    "OUTPUT_DIR = \"/workspace/stable-diffusion-webui/outputs\"\n",
    "\n",
    "# === Prompt ===\n",
    "co = cohere.Client(\"V2CGAFrhoVGvMmqOwhxrH7uimhIgR2S8787ODJD4\")\n",
    "\n",
    "transcript = \"\"\"\n",
    "Leon wants a cozy, beachy kitchen with an L-shaped layout. He loves blue and beige colors, natural wood elements, and is considering a beachy backsplash with mosaic tiles or sea glass. He's also interested in light fixtures made of natural materials like rattan or driftwood.\n",
    "\"\"\"\n",
    "\n",
    "# STEP 3: Build smart extraction prompt for LLM\n",
    "system_prompt = \"\"\"\n",
    "You are a kitchen prompt expert. Your job is to read a conversation transcript and generate a short, visually rich prompt that can be used with Stable Diffusion XL (SDXL).\n",
    "\n",
    "Rules:\n",
    "- Format: \"[Style] kitchen, [materials], [layout/furniture], [lighting], photorealistic\"\n",
    "- Do NOT use vague words like \"beautiful\" or \"cozy\"\n",
    "- Focus on visual elements only (e.g. colors, materials, objects, layout)\n",
    "- Max 77 tokens\n",
    "- No user names, no quotes, no assistant text\n",
    "\"\"\"\n",
    "\n",
    "full_prompt = system_prompt + \"\\n\\nTranscript:\\n\" + transcript + \"\\n\\nOutput:\"\n",
    "\n",
    "# STEP 4: Send to Cohere\n",
    "response = co.generate(\n",
    "    model=\"command-r-plus\",\n",
    "    prompt=full_prompt,\n",
    "    max_tokens=100,\n",
    "    temperature=0.6\n",
    ")\n",
    "\n",
    "# STEP 5: Print result\n",
    "PROMPT = response.generations[0].text.strip()\n",
    "print(\"\\nüéØ Final SDXL Prompt:\")\n",
    "print(PROMPT)\n",
    "NEGATIVE_PROMPT = \"blurry, low quality, distorted, granite countertops, generic kitchen, plain design\"\n",
    "\n",
    "# === Prepare output ===\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_run_dir = os.path.join(OUTPUT_DIR, f\"combined_run_{timestamp}\")\n",
    "os.makedirs(output_run_dir, exist_ok=True)\n",
    "\n",
    "# === Load ControlNet Models ===\n",
    "print(\"üîπ Loading ControlNet models...\")\n",
    "controlnet_canny = ControlNetModel.from_pretrained(CONTROLNET_CANNY_DIR, torch_dtype=torch.float16)\n",
    "controlnet_depth = ControlNetModel.from_pretrained(CONTROLNET_DEPTH_DIR, torch_dtype=torch.float16)\n",
    "\n",
    "# === Load VAE ===\n",
    "print(\"üîπ Loading custom VAE...\")\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    \"/workspace/stable-diffusion-webui/models/sd_xl_base_1.0/vae\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# === Load Tokenizers & Text Encoders ===\n",
    "print(\"üîπ Loading tokenizers and encoders...\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(f\"{MODEL_BASE}/tokenizer\")\n",
    "tokenizer_2 = CLIPTokenizer.from_pretrained(f\"{MODEL_BASE}/tokenizer_2\")\n",
    "\n",
    "text_encoder = CLIPTextModel.from_pretrained(f\"{MODEL_BASE}/text_encoder\", torch_dtype=torch.float16)\n",
    "text_encoder_2 = CLIPTextModelWithProjection.from_pretrained(\n",
    "    f\"{MODEL_BASE}/text_encoder_2\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# === Load SDXL Pipeline ===\n",
    "print(\"üîπ Loading SDXL pipeline...\")\n",
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    f\"{MODEL_BASE}/unet\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"\n",
    ")\n",
    "from diffusers import EulerDiscreteScheduler\n",
    "\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(\n",
    "    \"/workspace/stable-diffusion-webui/models/sd_xl_base_1.0/scheduler\"\n",
    ")\n",
    "\n",
    "from diffusers import StableDiffusionXLControlNetPipeline\n",
    "\n",
    "pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    pretrained_model_name_or_path=MODEL_BASE,\n",
    "    controlnet=[controlnet_canny, controlnet_depth],\n",
    "    unet=unet,\n",
    "    vae=vae,\n",
    "    text_encoder=text_encoder,\n",
    "    text_encoder_2=text_encoder_2,\n",
    "    tokenizer=tokenizer,\n",
    "    tokenizer_2=tokenizer_2,\n",
    "    scheduler=scheduler,\n",
    "    torch_dtype=torch.float16,\n",
    "    #variant=\"fp16\"\n",
    ")\n",
    "pipe.enable_attention_slicing()\n",
    "pipe.enable_vae_tiling()\n",
    "pipe.enable_model_cpu_offload()\n",
    "pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "from diffusers import StableDiffusionXLImg2ImgPipeline\n",
    "\n",
    "print(\"üîπ Loading SDXL Refiner pipeline...\")\n",
    "\n",
    "REFINER_DIR = \"/workspace/stable-diffusion-webui/models/sd_xl_refiner_1.0\"\n",
    "\n",
    "from diffusers import StableDiffusionXLImg2ImgPipeline\n",
    "\n",
    "print(\"üîπ Loading SDXL Refiner pipeline...\")\n",
    "refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "    \"/workspace/stable-diffusion-webui/models/sd_xl_refiner_1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\"\n",
    ")\n",
    "refiner.enable_model_cpu_offload()\n",
    "refiner.enable_attention_slicing()\n",
    "refiner.enable_vae_tiling()\n",
    "refiner.enable_model_cpu_offload()  # keep memory usage low\n",
    "\n",
    "# === Prepare input image ===\n",
    "print(\"üîπ Processing input image...\")\n",
    "init_image = load_image(INPUT_IMAGE).convert(\"RGB\")\n",
    "width, height = init_image.size\n",
    "\n",
    "# === Generate Canny map ===\n",
    "print(\"üîπ Creating Canny edges...\")\n",
    "gray = cv2.cvtColor(np.array(init_image), cv2.COLOR_RGB2GRAY)\n",
    "canny = cv2.Canny(gray, 100, 200)\n",
    "canny_image = Image.fromarray(cv2.cvtColor(canny, cv2.COLOR_GRAY2RGB))\n",
    "\n",
    "# === Simulated depth map ===\n",
    "print(\"üîπ Creating Depth map...\")\n",
    "depth = init_image.convert(\"L\")\n",
    "depth_array = cv2.equalizeHist(np.array(depth))\n",
    "depth_image = Image.fromarray(np.stack([depth_array] * 3, axis=-1))\n",
    "\n",
    "# === Save debug images ===\n",
    "init_image.save(os.path.join(output_run_dir, \"01_original.png\"))\n",
    "canny_image.save(os.path.join(output_run_dir, \"02_canny.png\"))\n",
    "depth_image.save(os.path.join(output_run_dir, \"03_depth.png\"))\n",
    "\n",
    "# === Generate result ===\n",
    "print(\"üé® Generating design with Canny + Depth ControlNet...\")\n",
    "# Truncate prompt to 77 tokens\n",
    "# üîê Truncate prompt to avoid token overflow (max 77 tokens)\n",
    "tokenized = tokenizer(PROMPT, return_tensors=\"pt\", truncation=True, max_length=77)\n",
    "PROMPT = tokenizer.decode(tokenized[\"input_ids\"][0], skip_special_tokens=True)\n",
    "print(f\"üß† Truncated prompt token count: {len(tokenized['input_ids'][0])}\")\n",
    "\n",
    "result = pipe(\n",
    "    prompt=PROMPT,\n",
    "    negative_prompt=NEGATIVE_PROMPT,\n",
    "    image=[canny_image, depth_image],\n",
    "    controlnet_conditioning_scale=[0.5, 0.2],\n",
    "    num_inference_steps=40,\n",
    "    guidance_scale=9.0,\n",
    "    width=width,\n",
    "    height=height\n",
    ").images[0]\n",
    "\n",
    "# === Save result ===\n",
    "# === Save result ===\n",
    "result_path = os.path.join(output_run_dir, \"04_result.png\")\n",
    "result.save(result_path)\n",
    "print(f\"‚úÖ Base result saved to: {result_path}\")\n",
    "\n",
    "# === Refine result with SDXL Refiner ===\n",
    "print(\"‚ú® Refining the image with SDXL Refiner...\")\n",
    "\n",
    "refined = refiner(\n",
    "    prompt=PROMPT,\n",
    "    negative_prompt=NEGATIVE_PROMPT,\n",
    "    image=result,\n",
    "    strength=0.3,\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=25\n",
    ").images[0]\n",
    "\n",
    "# === Save refined result ===\n",
    "refined_path = os.path.join(output_run_dir, \"05_refined.png\")\n",
    "refined.save(refined_path)\n",
    "print(f\"‚úÖ Refined result saved to: {refined_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
